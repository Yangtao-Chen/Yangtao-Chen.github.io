<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>FL-LLM综述</title>
    <link href="/2025/03/16/FL-LLM%E7%BB%BC%E8%BF%B0/"/>
    <url>/2025/03/16/FL-LLM%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="Towards-Federated-Large-Language-Models-Motivations-Methods-and-Future-Directions"><a href="#Towards-Federated-Large-Language-Models-Motivations-Methods-and-Future-Directions" class="headerlink" title="Towards Federated Large Language Models:  Motivations, Methods, and Future Directions"></a>Towards Federated Large Language Models:  Motivations, Methods, and Future Directions</h1><h3 id="联邦学习分类"><a href="#联邦学习分类" class="headerlink" title="联邦学习分类"></a>联邦学习分类</h3><p>根据客户端之间数据分布的性质，联邦学习通常分为三个不同的类别，包括Horizontal Federated Learning (HFL), Vertical Federated Learning (VFL), and Transfer Federated Learning (TFL)。</p><ul><li>HFL：各客户端之间特征重叠多，节点重叠少。常规的FL</li><li>VFL：在拥有一组通用样本但具有不同功能集的客户端网络之间协作学习共享 AI 模型。它采用实体对齐方法来合并来自这些客户的交叉样本，然后使用这些样本集体训练统一的 AI 模型。使用加密协议可以增强此过程的安全性。</li><li>TFL：是节点之间特征和样本重叠很少的情况下的首选策略。它涉及将异构特征空间的特征转换为统一格式，从而能够使用从众多客户端编译的数据来训练模型。然后，负责聚合的服务器根据从参与者的学习过程中收到的权重来更新模型。主要目标是开发对特定用例有效的定制模型，尤其是在数据可用性有限的情况下，从而代表 FL 策略中数据组织的另一个关键方面。</li></ul><h3 id="背景-FL-LLM"><a href="#背景-FL-LLM" class="headerlink" title="背景-FL-LLM"></a>背景-FL-LLM</h3><ol><li>LLM 训练数据的海量和分布式性质<br>LLM 通常使用大量高质量的数据进行预训练，以实现惊人的性能。例如，GPT-3 使用 45TB 的文本数据进行训练，而 Meta LLaMA-2 使用 20 万亿个令牌进行训练。然而，预计这种高质量的数据将在五年内耗尽。此外，以前提供免费公共数据的平台，如 Twitter，已经开始对访问其数据收取高额费用。此外，使用这些公共数据还可能涉及法律和版权相关的复杂性。使用公共数据集训练性能更好的 LLM 将变得越来越困难。<br>相反，大量数据仍然可以在私有域中访问，涵盖广泛的个人和公司来源。然而，聚合这些分布式私有数据集进行集中训练不仅需要复杂的数据集成工作，而且还会带来潜在的隐私风险。考虑到模型性能和效率，FL 是一个很有前途的解决方案。通过直接利用私有数据进行模型训练，它解决了隐私和跨各个域的数据分发带来的挑战。此外，通过使用 FL 进行训练，LLM 可以访问更广泛的数据以执行优化任务，例如微调、提示调整和预训练。增强的数据访问有助于创建更准确、更高效的 AI 系统，更好地满足用户在各种应用场景中的需求。</li><li>数据隐私<br>鉴于使用的大量分布式数据，数据隐私是 LLM 培训和应用中的一个关键问题。在 FL 中，服务器不需要原始数据进行训练。服务器和客户端仅在模型训练中交换中间信息，例如模型权重或梯度更新。这一核心思想确保敏感数据保存在本地并且不会泄露。因此，FL 降低了将敏感用户信息暴露给外部第三方的风险，并增强了 LLM 培训和应用程序中的数据隐私。</li><li>通过更新数据持续改进性能<br>另一个与数据相关的挑战是必须让 LLM 保持最新知识的更新。即使在很短的时间内，实际场景中的数据也会不断增长。例如，无人机和移动机器人等常见应用会随着时间的推移不断生成新数据。保持 LLM 的相关性和及时性变得具有挑战性，尤其是在处理分布式数据时。<br>各种来源的信息的动态性质需要不断调整和更新，以确保 LLM 保持准确和有效。FL 通过利用分布式和异构数据源实现模型的持续适应和增强，从而提供了一种解决方案。例如，LLM 可以以联合方式部署，其中本地模型根据特定于该位置的数据进行额外的微调。不传输本地数据，而是仅将模型的更新传输回中央服务器。这允许根据用户数据逐步增强全局模型，而无需直接访问该数据。</li><li>LLM 训练的高计算需求<br>训练大规模 LLM 需要大量的计算资源。这给缺乏独立进行 LLM 培训的必要框架或能力的独立实体带来了挑战。联邦学习促进了一种集体训练方法，允许实体结合他们的计算能力，这反过来又分散了训练工作量并减轻了任何单个实体的负担。另一方面，联邦学习在整个生命周期过程中通常涉及多个具有异构计算资源的本地客户端，它可以根据节点的计算能力调整原始模型，这样即使是计算能力较低的客户端也可以加入 LLM 训练和微调过程。</li><li>模型个性化和适应<br>随着 LLM 的快速发展，越来越多的大型模型正在向特定领域发展。这些应用场景不仅涉及不同的特定领域知识，而且对客户端的计算能力和硬件要求也有一定的约束。因此，由于 FL 的去中心化性质，它可以通过对用户生成的多样化数据进行训练，为用户提供个性化和自适应的 LLM 服务。另一个重要问题是 LLM 培训中的偏见。在 FL 中，模型从不同的用户那里学习，这使得 LLM 使用的数据和知识多样化。这有助于模型更好地理解现实世界场景的细微差别和复杂性，并为不同的任务和领域做出更明智、更少偏见的决策，从而有助于减少 LLM 系统中的偏差。</li></ol><h3 id="面临的挑战："><a href="#面临的挑战：" class="headerlink" title="面临的挑战："></a>面临的挑战：</h3><ol><li>计算和通信资源问题：为了应对这些资源挑战，可以采取多种策略。剪枝，量化和知识蒸馏等技术可以用来减少LLMs的大小，使其更适合于FL环境。</li><li>同步与协调：由于LLMs规模巨大，客户端在本地训练后需要向中心服务器发送大量的更新数据。这种大规模的数据传输不仅增加了通信开销，而且会导致更新的延迟和潜在的不稳定。停滞更新会减缓全局模型的收敛速度，降低其精度。一种方法是同步聚合，服务器在执行全局更新之前等待所有客户端的更新。然而，这可能会导致延迟增加，特别是当一些客户端具有较慢的网络连接或较低的计算能力时。另一方面，异步聚合允许服务器在收到来自任何客户端的更新后，立即更新全局模型。虽然这可以减少延迟，但它引入了合并过时更新的风险，这会降低模型的性能。为了缓解这种情况，可以使用诸如过期感知聚合等技术，其中服务器根据更新的及时性为其分配不同的权重，赋予最近更新更多的重要性。追逐客户是另一个需要解决的问题。这些都是延迟同步过程的较慢客户端。缓解这种情况的一种方法是设置更新的截止时间，在此之后服务器执行可用的更新，忽略掉掉队者。另一种方法是使用部分聚合，其中服务器聚合来自客户机子集的更新，确保同步过程不会被少数缓慢的客户机占用。</li><li>异质性：在FL中，数据分布在多个客户端上，每个客户端上的数据在数量、质量和分布上都可能存在巨大差异。这种数据的非IID性质会导致在训练LLMs时模型发散和性能次优，即一种被称为数据异质性的现象。</li><li>隐私和安全问题：LLMs需要应对各种潜在的攻击和偏见。如对抗样本，后门攻击，投毒攻击，模型窃取等。联邦网络中的词嵌入中毒攻击的影响是巨大的；即使是少数被妥协的客户端也会显著降低全局模型。在部署在无线网络上的联邦LLM系统中，对抗性干扰作为一种实际的威胁出现，破坏了传输过程中的敏感词嵌入。</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>-论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>fedllm代码调试</title>
    <link href="/2025/02/17/fedllm%E4%BB%A3%E7%A0%81%E8%B0%83%E8%AF%95/"/>
    <url>/2025/02/17/fedllm%E4%BB%A3%E7%A0%81%E8%B0%83%E8%AF%95/</url>
    
    <content type="html"><![CDATA[<h2 id="OpenFedLLM-Training-Large-Language-Models-on-Decentralized-Private-Data-via-Federated-Learning"><a href="#OpenFedLLM-Training-Large-Language-Models-on-Decentralized-Private-Data-via-Federated-Learning" class="headerlink" title="OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning"></a>OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning</h2><p>仓库地址：<a href="https://github.com/rui-ye/OpenFedLLM">https://github.com/rui-ye/OpenFedLLM</a></p><ol><li>运行指令</li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=1 python main_sft.py  --model_name_or_path <span class="hljs-string">&quot;dataroot/models/NousResearch/Llama-2-7b-hf&quot;</span>  --dataset_name <span class="hljs-string">&quot;lucasmccabe-lmi/CodeAlpaca-20k&quot;</span>  --dataset_sample 20000  --fed_alg <span class="hljs-string">&quot;fedavg&quot;</span>  --num_clients 20  --sample_clients 2  --max_steps 10  --num_rounds 200  --batch_size 16  --gradient_accumulation_steps 1  --seq_length 512  --peft_lora_r 32  --peft_lora_alpha 64  --use_peft  --load_in_8bit  --output_dir <span class="hljs-string">&quot;./output&quot;</span>  --template <span class="hljs-string">&quot;alpaca&quot;</span> <br><br>这里修改了--model_name_or_path <span class="hljs-string">&quot;dataroot/models/NousResearch/Llama-2-7b-hf&quot;</span><br>修改了--dataset_name <span class="hljs-string">&quot;lucasmccabe-lmi/CodeAlpaca-20k&quot;</span><br>这两个是已下载的模型和数据集<br></code></pre></td></tr></table></figure><p>同时因为是已有的数据集，所以要修改代码utils&#x2F;process_dataset.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># dataset = load_dataset(dataset_name, split=&quot;train&quot;)</span><br>dataset = load_from_disk(dataset_name)<br></code></pre></td></tr></table></figure><p>CUDA_VISIBLE_DEVICES&#x3D;$gpu python main_dpo.py <br> –model_name_or_path ehartford&#x2F;Wizard-Vicuna-7B-Uncensored <br> –dataset_name Anthropic&#x2F;hh-rlhf <br> –dataset_sample 20000 <br> –fed_alg “fedavg” <br> –num_clients 5 <br> –sample_clients 2 <br> –learning_rate 5e-4 <br> –max_steps 10 <br> –num_rounds 200 <br> –batch_size 16 <br> –gradient_accumulation_steps 1 <br> –seq_length 512 <br> –use_peft <br> –load_in_8bit <br> –output_dir .&#x2F;output <br> –template “vicuna_v1.1” \</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">watch -n <span class="hljs-number">1</span> nvidia-smi<br><br><span class="hljs-keyword">screen </span>-S <span class="hljs-keyword">jcx </span>新建一个名为<span class="hljs-keyword">jcx的屏幕</span><br><span class="hljs-keyword"></span><span class="hljs-keyword">screen </span>-ls查看pid号：<span class="hljs-number">9889</span><br>下次使用这个屏幕：<span class="hljs-keyword">screen </span>-r <span class="hljs-number">9889</span><br>lsof -i:<span class="hljs-number">5000</span>查看指定port<br><span class="hljs-keyword">screen </span>-D -r <span class="hljs-keyword">jcx重新显示</span><br><span class="hljs-keyword"></span><br>free查看空闲内存<br>free -m以MB为单位展示<br>top S睡眠，R运行，T跟踪，Z僵尸进程<br>htop查看内存<br>ps u可以查看进程<br>kill -<span class="hljs-number">9</span> 进程号<br>ps aux<span class="hljs-title">|grep pheimg</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>联邦学习</category>
      
      <category>大模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>论文代码调试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>常见问题</title>
    <link href="/2025/02/16/%E6%96%87%E7%AB%A0%E6%9B%B4%E6%96%B0/"/>
    <url>/2025/02/16/%E6%96%87%E7%AB%A0%E6%9B%B4%E6%96%B0/</url>
    
    <content type="html"><![CDATA[<h2 id="页面的生成"><a href="#页面的生成" class="headerlink" title="页面的生成"></a>页面的生成</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new <span class="hljs-string">&quot;pageName&quot;</span> <span class="hljs-comment">#新建文章</span><br>hexo new page <span class="hljs-string">&quot;pageName&quot;</span> <span class="hljs-comment">#新建页面</span><br>hexo g <span class="hljs-comment">#生成</span><br>hexo d <span class="hljs-comment">#部署</span><br>hexo g -d <span class="hljs-comment">#生成并部署</span><br>hexo s <span class="hljs-comment">#本地预览</span><br>hexo clean <span class="hljs-comment">#清除缓存和生成文件</span><br>hexo <span class="hljs-built_in">help</span> <span class="hljs-comment">#查看帮助</span><br><br></code></pre></td></tr></table></figure><p>发布文章的动作</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash">hexo new <span class="hljs-string">&quot;pageName&quot;</span> <span class="hljs-comment">#新建文章</span><br>hexo clean <span class="hljs-comment">#清除缓存和生成文件</span><br>hexo g -d <span class="hljs-comment">#生成并部署</span><br></code></pre></td></tr></table></figure><h2 id="感谢大佬的教程"><a href="#感谢大佬的教程" class="headerlink" title="感谢大佬的教程"></a>感谢大佬的教程</h2><ol><li><p>搭建网站<br><a href="https://blog.csdn.net/yaorongke/article/details/119089190">https://blog.csdn.net/yaorongke/article/details/119089190</a></p></li><li><p>fluid主题美化<br><a href="https://mrna16.github.io/2024/11/14/%E3%80%90Hexo%E3%80%91Fluid%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/">https://mrna16.github.io/2024/11/14/%E3%80%90Hexo%E3%80%91Fluid%E4%B8%BB%E9%A2%98%E7%BE%8E%E5%8C%96/</a></p></li></ol>]]></content>
    
    
    <categories>
      
      <category>日常</category>
      
    </categories>
    
    
    <tags>
      
      <tag>原创</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
